# cmonit - Central Monit Monitoring System
## Project Plan

## Overview

**cmonit** is an open-source clone of the proprietary M/Monit software that provides centralized monitoring and management of all Monit-enabled hosts via a modern, clean, and mobile-friendly web interface.

## Technology Stack

### Backend
- **Language**: Go (Golang) - KISS principle, excellent for HTTP services and concurrency
- **Database**: SQLite - Simple, embedded, serverless, perfect for this use case
- **HTTP Framework**: Standard library `net/http` with `gorilla/mux` for routing (optional)

### Frontend
- **CSS Framework**: Tailwind CSS (loaded via CDN)
- **Charting**: Chart.js (loaded via CDN)
- **Templates**: Go `html/template` for server-side rendering

### Why This Stack?

**Go Advantages**:
- Single binary deployment (no dependencies)
- Excellent HTTP and concurrent request handling
- Built-in templating
- Strong standard library
- Cross-platform compilation
- Low memory footprint

**SQLite Advantages**:
- No separate database server needed
- Zero configuration
- ACID compliant
- Perfect for embedded systems
- File-based (easy backup/migration)

**Tailwind + Chart.js**:
- No build step required (CDN-based)
- Modern, responsive design out of the box
- Chart.js is simple and lightweight
- Mobile-friendly by default

## Project Structure

```
cmonit/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îî‚îÄ‚îÄ cmonit/
‚îÇ       ‚îî‚îÄ‚îÄ main.go          # Entry point
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ collector.go     # /collector endpoint (receives from Monit)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.go           # M/Monit-compatible HTTP API
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handlers.go      # HTTP handlers
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.go        # Database schema
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.go        # Data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queries.go       # Database queries
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migrations.go    # Schema initialization
‚îÇ   ‚îú‚îÄ‚îÄ parser/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ xml.go           # Monit XML parser
‚îÇ   ‚îú‚îÄ‚îÄ web/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.go        # Web server setup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handlers.go      # Web UI handlers
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îî‚îÄ‚îÄ config.go        # Configuration management
‚îú‚îÄ‚îÄ web/
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.html        # Base template
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard.html   # Dashboard view
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ host.html        # Single host view
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ graphs.html      # Time-series graphs
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îî‚îÄ‚îÄ (optional custom CSS/JS)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ monit-collector-protocol.md
‚îÇ   ‚îú‚îÄ‚îÄ project-plan.md
‚îÇ   ‚îú‚îÄ‚îÄ api-compatibility.md
‚îÇ   ‚îî‚îÄ‚îÄ testing-plan.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ go.mod
‚îú‚îÄ‚îÄ go.sum
‚îú‚îÄ‚îÄ Makefile
‚îî‚îÄ‚îÄ .gitignore
```

## Development Phases

### Phase 1: Collector Daemon (Foundation)

**Goal**: Create HTTP server that receives and stores Monit status updates

**Components**:
1. Basic Go HTTP server listening on port 8080
2. `/collector` endpoint accepting POST requests
3. HTTP Basic Auth validation (username: monit, password: monit)
4. XML parser for Monit status format
5. SQLite database with initial schema
6. Data insertion logic

**Database Schema (Current - Version 4)**:

The database schema has evolved through automatic migrations:

- **Schema v1**: Initial schema with hosts, services, metrics, events tables
- **Schema v2**: Added filesystem_metrics table for detailed filesystem monitoring
- **Schema v3**: Added network_metrics table for network interface monitoring
- **Schema v4** (Current): Added file_metrics and program_metrics tables

See `internal/db/schema.go` for the complete current schema including:

**Core Tables**:
- `hosts` - Monitored host information with platform details and Monit daemon info
- `services` - Services monitored on each host (all types)
- `events` - Service state change events and Monit restart detection

**Metrics Tables**:
- `metrics` - Time-series system/process CPU, memory, load metrics
- `filesystem_metrics` - Block/inode usage, filesystem type, I/O operations
- `network_metrics` - Link state, speed, duplex, traffic statistics
- `file_metrics` - File mode, ownership, timestamps, checksums (Type 2 services)
- `program_metrics` - Program execution status and output (Type 7 services)

**Schema Management**:
- `schema_version` - Tracks current schema version with automatic migrations
- All migrations are applied automatically on startup

**Acceptance Tests**:
- [ ] Server starts and listens on port 8080
- [ ] `/collector` endpoint responds to POST requests
- [ ] Rejects requests without valid Basic Auth
- [ ] Accepts requests with monit:monit credentials
- [ ] Parses XML from existing Monit agent
- [ ] Creates database file if it doesn't exist
- [ ] Inserts host record on first contact
- [ ] Updates host last_seen on each contact
- [ ] Inserts/updates service records
- [ ] Stores metrics in time-series table
- [ ] Returns HTTP 200 with `Server: cmonit/0.1` header
- [ ] Handles gzip-compressed requests (if sent by Monit)

**Deliverable**: Working daemon that collects data from the running Monit agent

---

### Phase 2: Web UI - Dashboard

**Goal**: Display current status of all monitored hosts and services

**Components**:
1. Web server on port 3000 (separate from collector)
2. Dashboard page showing all hosts
3. Service status table per host
4. Basic styling with Tailwind CSS
5. Auto-refresh every 30 seconds

**Dashboard Features**:
- List of all monitored hosts
- For each host:
  - Hostname
  - Last seen timestamp
  - Overall health status
  - Number of services
- For each service:
  - Service name
  - Service type
  - Current status (OK, Failed, etc.)
  - Monitor status
  - Key metrics (CPU%, Memory%, etc.)

**Acceptance Tests**:
- [ ] Web server starts on port 3000
- [ ] Dashboard accessible at http://localhost:3000/
- [ ] Shows all monitored hosts
- [ ] Shows all services for each host
- [ ] Displays current status correctly
- [ ] Shows correct timestamps
- [ ] Auto-refreshes every 30 seconds
- [ ] Responsive design works on mobile
- [ ] Color-coded status indicators (green=OK, red=failed, yellow=warning)
- [ ] No JavaScript errors in console

**Deliverable**: Functional web dashboard showing real-time status

---

### Phase 3: Web UI - Time-Series Graphs

**Goal**: Visualize metrics over time using Chart.js

**Components**:
1. Graphs page for each host/service
2. Chart.js integration
3. Query optimization for time-series data
4. Time range selector (1h, 6h, 24h, 7d, 30d)

**Graph Types**:
- **System metrics**:
  - CPU usage over time (user, system, nice, wait)
  - Memory usage over time
  - Swap usage over time
  - Load average over time
  - Disk space usage over time (per filesystem)
  - Network traffic over time (per interface)

- **Process metrics**:
  - CPU% over time
  - Memory usage over time
  - Thread count over time

**Acceptance Tests**:
- [ ] Graphs page accessible for each host
- [ ] CPU usage graph displays correctly
- [ ] Memory usage graph displays correctly
- [ ] Disk space graph displays correctly
- [ ] Load average graph displays correctly
- [ ] Time range selector works (1h, 6h, 24h, 7d, 30d)
- [ ] Graphs update when time range changes
- [ ] Hover tooltips show exact values
- [ ] Legend works correctly
- [ ] Graphs are responsive on mobile
- [ ] Data points are interpolated correctly
- [ ] No performance issues with large datasets

**Deliverable**: Interactive time-series graphs for all metrics

---

### Phase 4: M/Monit API Compatibility

**Goal**: Implement M/Monit HTTP API for compatibility with existing tools

**API Endpoints** (from https://mmonit.com/documentation/http-api/):

Authentication & Session:
- `POST /login` - User login
- `GET /logout` - User logout

Status & Monitoring:
- `GET /status/hosts/:id` - Get host status
- `GET /status/hosts/:id/services` - Get all services for a host
- `GET /status/hosts/:id/services/:name` - Get specific service status
- `GET /uptime/hosts/:id` - Get uptime data
- `GET /reports/uptime/hosts/:id` - Get uptime reports

Events:
- `GET /events/list` - List events
- `GET /events/get/:id` - Get specific event
- `DELETE /events/delete/:id` - Delete event

Administrative:
- `GET /admin/hosts` - List all hosts
- `POST /admin/hosts` - Add new host
- `DELETE /admin/hosts/:id` - Remove host
- `PUT /admin/hosts/:id` - Update host
- `GET /admin/groups` - List groups
- `POST /admin/groups` - Create group

**Acceptance Tests**:
- [ ] All API endpoints respond correctly
- [ ] JSON responses match M/Monit format
- [ ] Authentication works
- [ ] Error responses have correct format
- [ ] API documentation is complete
- [ ] Integration tests pass

**Deliverable**: M/Monit-compatible REST API

---

### Phase 5: Advanced Features

**Additional features to consider**:

1. **Alerting**:
   - Email notifications
   - Webhook support
   - Alert rules

2. **Multi-host support**:
   - Host groups
   - Filtering and search

3. **User management**:
   - Multiple users
   - Role-based access control
   - API tokens

4. **Data retention**:
   - Automatic cleanup of old metrics
   - Configurable retention policies
   - Data aggregation for long-term storage

5. **Export**:
   - CSV export of metrics
   - PDF reports

---

## Testing Strategy

### Unit Tests
- XML parser tests with sample Monit data
- Database query tests
- HTTP handler tests

### Integration Tests
- End-to-end flow from Monit ‚Üí collector ‚Üí database ‚Üí UI
- API compatibility tests

### Manual Tests
- Real Monit agent sending data
- UI testing on different browsers
- Mobile responsiveness testing

### Performance Tests
- Multiple Monit agents (10, 50, 100 hosts)
- Long-running stability test (24h+)
- Database growth over time

---

## Deployment

### Binary Distribution
```bash
# Build for current platform
make build

# Build for all platforms
make build-all

# Output:
# bin/cmonit-linux-amd64
# bin/cmonit-freebsd-amd64
# bin/cmonit-darwin-amd64
```

### Running
```bash
# Start with default settings
./cmonit

# Custom config
./cmonit --config /path/to/config.yaml

# Specify database location
./cmonit --db /path/to/cmonit.db

# Change ports
./cmonit --collector-port 8080 --web-port 3000
```

### Configuration File (cmonit.yaml)
```yaml
collector:
  port: 8080
  auth:
    username: monit
    password: monit
  compression: true

web:
  port: 3000
  refresh_interval: 30

database:
  path: ./cmonit.db
  retention_days: 30

logging:
  level: info
  format: json
```

---

## Development Workflow

### Day 1-2: Phase 1 - Collector Setup
1. Initialize Go module
2. Set up project structure
3. Create database schema
4. Implement XML parser
5. Build collector endpoint
6. Test with real Monit agent

### Day 3-4: Phase 2 - Dashboard
1. Set up web server
2. Create HTML templates
3. Implement dashboard handlers
4. Add Tailwind CSS styling
5. Test UI

### Day 5-6: Phase 3 - Graphs
1. Implement graph data queries
2. Create graph templates
3. Integrate Chart.js
4. Add time range selectors
5. Test graphs

### Day 7-8: Phase 4 - API
1. Design API routes
2. Implement API handlers
3. Add authentication
4. Write API tests
5. Document API

### Day 9-10: Polish & Documentation
1. Add README
2. Write user documentation
3. Create deployment guide
4. Performance testing
5. Bug fixes

---

## Success Criteria

The project will be considered successful when:

1. ‚úÖ cmonit receives and stores data from at least one Monit agent
2. ‚úÖ Web dashboard displays real-time status of all monitored services
3. ‚úÖ Time-series graphs show historical metrics
4. ‚úÖ M/Monit API endpoints return correct data
5. ‚úÖ All acceptance tests pass
6. ‚úÖ System runs stably for 24+ hours
7. ‚úÖ Documentation is complete
8. ‚úÖ Single binary deployment works on FreeBSD, Linux, and macOS

---

## Alternative Considerations

### Why NOT use other languages?

**Python**:
- ‚ùå Requires Python runtime
- ‚ùå Dependency management complexity
- ‚ùå Slower than Go for HTTP services
- ‚úÖ Easier for some developers

**Node.js**:
- ‚ùå Requires Node.js runtime
- ‚ùå Callback hell / async complexity
- ‚ùå npm dependency bloat
- ‚úÖ Good ecosystem for web

**Rust**:
- ‚ùå Steeper learning curve
- ‚ùå Longer compilation times
- ‚ùå More verbose
- ‚úÖ Ultimate performance
- ‚úÖ Memory safety

**Conclusion**: Go provides the best balance of simplicity, performance, and deployment convenience for this project.

### Why NOT use PostgreSQL/MySQL?

- ‚ùå Requires separate database server
- ‚ùå Additional configuration complexity
- ‚ùå Overkill for single-server deployment
- ‚úÖ Better for very large deployments (1000+ hosts)

SQLite is perfect for small-to-medium deployments (<100 hosts). If scaling becomes an issue, migration to PostgreSQL can be done later.

---

## Testing with Multiple Monit Clients

### Option 1: Docker Containers
Create multiple Docker containers running Monit, each configured to send to cmonit:

```bash
# Create 5 test monit instances
for i in {1..5}; do
    docker run -d --name monit$i \
        -e MMONIT_URL=http://monit:monit@host.docker.internal:8080/collector \
        custom-monit-image
done
```

### Option 2: Multiple VMs/Jails
- Use FreeBSD jails or VMs
- Install Monit in each
- Configure each to point to cmonit

### Option 3: Mock Data Generator
Create a Go tool that simulates multiple Monit agents sending data:

```bash
# Simulate 10 hosts
./mock-monit --hosts 10 --interval 30 --url http://localhost:8080/collector
```

**Recommendation**: Start with the single existing Monit agent for Phase 1-2. Add multiple test clients in Phase 3 for load testing.

---

## Risk Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| XML parsing complexity | High | Use encoding/xml package, extensive testing |
| Database performance | Medium | Proper indexing, connection pooling |
| UI responsiveness | Low | Use Tailwind, test on mobile |
| API compatibility | High | Reference M/Monit docs, integration tests |
| Memory leaks | Medium | Profiling, load testing |
| Concurrent writes | Medium | Use SQLite WAL mode, proper locking |

---

## Phase 5: Host Lifecycle Management

**Goal**: Intelligent stale host detection and host deletion functionality for dynamic infrastructure

### Features

**1. Heartbeat-Based Health Status**
- Store poll interval from Monit (typically 30s)
- Three-state health indicator:
  - üü¢ Green (Healthy): `last_seen < poll_interval * 2`
  - üü° Yellow (Warning): `poll_interval * 2 <= last_seen < poll_interval * 5`
  - üî¥ Red (Offline): `last_seen >= poll_interval * 5`
- Visual status on dashboard with "last seen" timestamp
- No false alarms during maintenance windows

**2. Host Deletion with Safety**
- Delete button for offline hosts (>1 hour)
- Confirmation dialog requiring hostname entry
- Shows deletion impact (services, metrics, events count)
- Cascade deletion across all related tables
- Safety: Cannot delete recently active hosts

**3. Future Enhancements**
- Soft delete/archive functionality (Phase 2)
- Host lifecycle tags: production, ephemeral, testing, staging (Phase 2)
- Auto-archive policies for ephemeral hosts (Phase 3)
- Batch operations for multiple hosts (Phase 3)
- Data export before deletion (Phase 3)

### Database Schema Changes (Schema v5)

```sql
ALTER TABLE hosts ADD COLUMN poll_interval INTEGER DEFAULT 30;
```

**Future schema additions** (Phase 2+):
```sql
ALTER TABLE hosts ADD COLUMN archived INTEGER DEFAULT 0;
ALTER TABLE hosts ADD COLUMN archived_at DATETIME;
ALTER TABLE hosts ADD COLUMN lifecycle TEXT DEFAULT 'production';
```

### Implementation Phases

**Phase 1 (Essential) - Status Tracking:**
- [x] Store poll_interval in database
- [x] Add schema v7 migration (includes poll_interval field)
- [x] Implement health status calculation helper
- [x] Add health indicator to dashboard (üü¢üü°üî¥)
- [x] Show "last seen" with human-readable time
- [x] Add DELETE /admin/hosts/:id API endpoint
- [x] Add cascade deletion function
- [x] Add delete confirmation UI with hostname verification

**Known Issues Fixed:**
- ~~Bug: DELETE endpoint returned HTTP 500 due to incorrect DATETIME-to-integer conversion~~ Fixed in `/internal/db/storage.go:1170` using `CAST(strftime('%s', last_seen) AS INTEGER)`

**Phase 2 (Enhanced) - Archive System:**
- [ ] Add archived flag and timestamps
- [ ] Implement archive/restore functionality
- [ ] Show deletion impact statistics
- [ ] Add "Archived Hosts" page
- [ ] Host lifecycle tags

**Phase 3 (Advanced) - Automation:**
- [ ] Auto-archive policies based on lifecycle
- [ ] Batch operations (archive/delete multiple)
- [ ] Data export before deletion
- [ ] Email notifications before auto-archive

### Acceptance Tests

Phase 1:
- [x] Poll interval correctly stored from XML
- [x] Health status calculates correctly (green/yellow/red)
- [x] Dashboard shows visual health indicators
- [x] Cannot delete host active within last hour
- [x] Delete requires correct hostname confirmation
- [x] Deletion cascades to all related tables
- [x] Metrics/events/services deleted with host
- [x] Database integrity maintained after deletion

---

## Phase 6: Enhanced System Metrics Display

**Goal**: Display comprehensive system metrics (CPU breakdown, load average, memory, swap) in the Service Details page for System type services

### Current State

The System service type (type=5) currently shows minimal metrics in the service detail view. However, Monit agents already report comprehensive system information including:

**Available Data** (from XML parser in `internal/parser/xml.go`):
- **Load Average**: `Avg01`, `Avg05`, `Avg15` - 1min, 5min, and 15min load averages
- **CPU Usage**: `User`, `System`, `Nice`, `HardIRQ`, `Wait` (Linux also includes: IOWait, SoftIRQ, Steal, Guest, GuestNice)
- **Memory Usage**: `Percent`, `Kilobyte` - RAM usage percentage and absolute KB
- **Swap Usage**: `Percent`, `Kilobyte` - Swap usage percentage and absolute KB

**Current Database Storage** (`metrics` table):
- System metrics ARE being stored: `load_1min`, `load_5min`, `load_15min`, `cpu_user`, `cpu_sys`, `cpu_nice`, `cpu_wait`, `memory_pct`, `memory_kb`, `swap_pct`, `swap_kb`

**Problem**: The service detail page (`templates/service.html`) only displays basic information and doesn't show these comprehensive system metrics.

### Requirements

**User Request**: "Display comprehensive system metrics in the Service Details page including:
- Load average (3 values: 1min, 5min, 15min)
- CPU usage breakdown (varies by OS):
  - Linux: %usr, %sys, %nice, %iowait, %hardirq, %softirq, %steal, %guest, %guestnice
  - FreeBSD: %usr, %sys, %nice, %hardirq
- Memory usage (percentage and formatted size)
- Swap usage (percentage and formatted size)"

### Implementation Plan

**Phase 1: Backend Data Flow** ‚úì (Already Complete)
- [x] XML parser captures all system metrics (`internal/parser/xml.go`)
- [x] Storage layer saves metrics to database (`internal/db/storage.go`)
- [x] Service detail handler passes data to template (`internal/web/handlers.go`)

**Phase 2: Template Enhancement** (To Implement)
1. Read current `templates/service.html` structure
2. Identify System service section (type=5)
3. Add comprehensive metrics display sections:
   - **Load Average Section**: Display 3 values with visual indicators
   - **CPU Usage Breakdown**: Show all available CPU metrics with percentage bars
   - **Memory Usage**: Display percentage + formatted size (KB/MB/GB conversion)
   - **Swap Usage**: Display percentage + formatted size
4. Add CSS styling for visual clarity
5. Ensure responsive design (mobile-friendly)

**Phase 3: Handler Data Preparation** (To Verify)
1. Check `internal/web/handlers.go` service detail handler
2. Verify all system metrics are being passed to template
3. Add any missing data queries for System services
4. Ensure metrics are queried from database correctly

**Phase 4: UI/UX Design Considerations**
- Group related metrics into clear sections
- Use progress bars for percentages (CPU, memory, swap)
- Format large numbers (KB ‚Üí MB/GB) for readability
- Add tooltips for technical terms
- Use color coding (green=low, yellow=medium, red=high)
- Ensure consistency with existing UI design patterns

### Files to Modify

1. **`templates/service.html`** (Primary)
   - Add System metrics display sections
   - Update HTML structure with proper Tailwind CSS classes
   - Add metric visualization (progress bars, badges)

2. **`internal/web/handlers.go`** (If needed)
   - Verify `handleServiceDetail()` function
   - Ensure System metrics are queried and passed to template
   - Add helper functions for data formatting if needed

3. **`internal/db/queries.go`** (If needed)
   - Verify system metrics query functions
   - Add any missing queries for historical data

### Acceptance Tests

**Functional Tests**:
- [ ] Service detail page displays all load average values (1min, 5min, 15min)
- [ ] CPU usage breakdown shows all available metrics for the OS
- [ ] Memory usage displays both percentage and formatted size
- [ ] Swap usage displays both percentage and formatted size
- [ ] Metrics update when page refreshes
- [ ] N/A displayed when metrics are not available
- [ ] Values are formatted correctly (e.g., "1.2 GB" not "1234567 KB")

**Visual Tests**:
- [ ] Metrics are clearly grouped and labeled
- [ ] Progress bars display correctly for percentages
- [ ] Layout is responsive on mobile devices
- [ ] Color coding is consistent with rest of application
- [ ] No visual glitches or layout issues

**Integration Tests**:
- [ ] FreeBSD system metrics display correctly
- [ ] Linux system metrics display correctly (if available)
- [ ] Historical graphs include new metrics (future enhancement)
- [ ] API endpoints return system metrics (existing functionality)

### Implementation Steps

**Step 1**: Read and analyze current `templates/service.html`
- Understand existing template structure
- Identify System service display section
- Note current styling patterns

**Step 2**: Design metrics layout
- Sketch sections for Load, CPU, Memory, Swap
- Decide on visualization approach (progress bars, badges, etc.)
- Plan responsive design

**Step 3**: Implement template changes
- Add Load Average section
- Add CPU Usage Breakdown section
- Add Memory Usage section
- Add Swap Usage section
- Apply consistent styling

**Step 4**: Verify handler data flow
- Check that all metrics are passed to template
- Add any missing database queries
- Test with live data

**Step 5**: Test and refine
- Test on FreeBSD system (primary environment)
- Verify all metrics display correctly
- Check mobile responsiveness
- Fix any issues

### Platform Considerations

**FreeBSD** (Primary development environment):
- CPU metrics: User, System, Nice, HardIRQ
- No IOWait, SoftIRQ, Steal, Guest, GuestNice

**Linux** (Secondary support):
- Full CPU breakdown including IOWait, SoftIRQ, Steal, Guest, GuestNice
- Template should handle both cases gracefully

**Display Strategy**:
- Show all metrics that are available
- Display "N/A" or hide sections for unavailable metrics
- Use conditional rendering in template

### Future Enhancements (Post-Implementation)

- Historical graphs for system metrics (Phase 3 enhancement)
- Alert thresholds for system metrics
- Comparative view across multiple hosts
- Export system metrics to CSV
- System health score calculation
- Predictive analysis (trend detection)

---

## Future Enhancements

- Prometheus exporter
- Grafana integration
- Clustering support (multiple cmonit instances)
- InfluxDB backend option
- Docker/Kubernetes native monitoring
- Mobile app
- Slack/Discord/Telegram notifications
- Incident management workflow
